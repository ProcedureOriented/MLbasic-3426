{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.数据集加载和预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
      "       [4.9, 3. , 1.4, 0.2],\n",
      "       [4.7, 3.2, 1.3, 0.2],\n",
      "       [4.6, 3.1, 1.5, 0.2],\n",
      "       [5. , 3.6, 1.4, 0.2],\n",
      "       [5.4, 3.9, 1.7, 0.4],\n",
      "       [4.6, 3.4, 1.4, 0.3],\n",
      "       [5. , 3.4, 1.5, 0.2],\n",
      "       [4.4, 2.9, 1.4, 0.2],\n",
      "       [4.9, 3.1, 1.5, 0.1],\n",
      "       [5.4, 3.7, 1.5, 0.2],\n",
      "       [4.8, 3.4, 1.6, 0.2],\n",
      "       [4.8, 3. , 1.4, 0.1],\n",
      "       [4.3, 3. , 1.1, 0.1],\n",
      "       [5.8, 4. , 1.2, 0.2],\n",
      "       [5.7, 4.4, 1.5, 0.4],\n",
      "       [5.4, 3.9, 1.3, 0.4],\n",
      "       [5.1, 3.5, 1.4, 0.3],\n",
      "       [5.7, 3.8, 1.7, 0.3],\n",
      "       [5.1, 3.8, 1.5, 0.3],\n",
      "       [5.4, 3.4, 1.7, 0.2],\n",
      "       [5.1, 3.7, 1.5, 0.4],\n",
      "       [4.6, 3.6, 1. , 0.2],\n",
      "       [5.1, 3.3, 1.7, 0.5],\n",
      "       [4.8, 3.4, 1.9, 0.2],\n",
      "       [5. , 3. , 1.6, 0.2],\n",
      "       [5. , 3.4, 1.6, 0.4],\n",
      "       [5.2, 3.5, 1.5, 0.2],\n",
      "       [5.2, 3.4, 1.4, 0.2],\n",
      "       [4.7, 3.2, 1.6, 0.2],\n",
      "       [4.8, 3.1, 1.6, 0.2],\n",
      "       [5.4, 3.4, 1.5, 0.4],\n",
      "       [5.2, 4.1, 1.5, 0.1],\n",
      "       [5.5, 4.2, 1.4, 0.2],\n",
      "       [4.9, 3.1, 1.5, 0.2],\n",
      "       [5. , 3.2, 1.2, 0.2],\n",
      "       [5.5, 3.5, 1.3, 0.2],\n",
      "       [4.9, 3.6, 1.4, 0.1],\n",
      "       [4.4, 3. , 1.3, 0.2],\n",
      "       [5.1, 3.4, 1.5, 0.2],\n",
      "       [5. , 3.5, 1.3, 0.3],\n",
      "       [4.5, 2.3, 1.3, 0.3],\n",
      "       [4.4, 3.2, 1.3, 0.2],\n",
      "       [5. , 3.5, 1.6, 0.6],\n",
      "       [5.1, 3.8, 1.9, 0.4],\n",
      "       [4.8, 3. , 1.4, 0.3],\n",
      "       [5.1, 3.8, 1.6, 0.2],\n",
      "       [4.6, 3.2, 1.4, 0.2],\n",
      "       [5.3, 3.7, 1.5, 0.2],\n",
      "       [5. , 3.3, 1.4, 0.2],\n",
      "       [7. , 3.2, 4.7, 1.4],\n",
      "       [6.4, 3.2, 4.5, 1.5],\n",
      "       [6.9, 3.1, 4.9, 1.5],\n",
      "       [5.5, 2.3, 4. , 1.3],\n",
      "       [6.5, 2.8, 4.6, 1.5],\n",
      "       [5.7, 2.8, 4.5, 1.3],\n",
      "       [6.3, 3.3, 4.7, 1.6],\n",
      "       [4.9, 2.4, 3.3, 1. ],\n",
      "       [6.6, 2.9, 4.6, 1.3],\n",
      "       [5.2, 2.7, 3.9, 1.4],\n",
      "       [5. , 2. , 3.5, 1. ],\n",
      "       [5.9, 3. , 4.2, 1.5],\n",
      "       [6. , 2.2, 4. , 1. ],\n",
      "       [6.1, 2.9, 4.7, 1.4],\n",
      "       [5.6, 2.9, 3.6, 1.3],\n",
      "       [6.7, 3.1, 4.4, 1.4],\n",
      "       [5.6, 3. , 4.5, 1.5],\n",
      "       [5.8, 2.7, 4.1, 1. ],\n",
      "       [6.2, 2.2, 4.5, 1.5],\n",
      "       [5.6, 2.5, 3.9, 1.1],\n",
      "       [5.9, 3.2, 4.8, 1.8],\n",
      "       [6.1, 2.8, 4. , 1.3],\n",
      "       [6.3, 2.5, 4.9, 1.5],\n",
      "       [6.1, 2.8, 4.7, 1.2],\n",
      "       [6.4, 2.9, 4.3, 1.3],\n",
      "       [6.6, 3. , 4.4, 1.4],\n",
      "       [6.8, 2.8, 4.8, 1.4],\n",
      "       [6.7, 3. , 5. , 1.7],\n",
      "       [6. , 2.9, 4.5, 1.5],\n",
      "       [5.7, 2.6, 3.5, 1. ],\n",
      "       [5.5, 2.4, 3.8, 1.1],\n",
      "       [5.5, 2.4, 3.7, 1. ],\n",
      "       [5.8, 2.7, 3.9, 1.2],\n",
      "       [6. , 2.7, 5.1, 1.6],\n",
      "       [5.4, 3. , 4.5, 1.5],\n",
      "       [6. , 3.4, 4.5, 1.6],\n",
      "       [6.7, 3.1, 4.7, 1.5],\n",
      "       [6.3, 2.3, 4.4, 1.3],\n",
      "       [5.6, 3. , 4.1, 1.3],\n",
      "       [5.5, 2.5, 4. , 1.3],\n",
      "       [5.5, 2.6, 4.4, 1.2],\n",
      "       [6.1, 3. , 4.6, 1.4],\n",
      "       [5.8, 2.6, 4. , 1.2],\n",
      "       [5. , 2.3, 3.3, 1. ],\n",
      "       [5.6, 2.7, 4.2, 1.3],\n",
      "       [5.7, 3. , 4.2, 1.2],\n",
      "       [5.7, 2.9, 4.2, 1.3],\n",
      "       [6.2, 2.9, 4.3, 1.3],\n",
      "       [5.1, 2.5, 3. , 1.1],\n",
      "       [5.7, 2.8, 4.1, 1.3],\n",
      "       [6.3, 3.3, 6. , 2.5],\n",
      "       [5.8, 2.7, 5.1, 1.9],\n",
      "       [7.1, 3. , 5.9, 2.1],\n",
      "       [6.3, 2.9, 5.6, 1.8],\n",
      "       [6.5, 3. , 5.8, 2.2],\n",
      "       [7.6, 3. , 6.6, 2.1],\n",
      "       [4.9, 2.5, 4.5, 1.7],\n",
      "       [7.3, 2.9, 6.3, 1.8],\n",
      "       [6.7, 2.5, 5.8, 1.8],\n",
      "       [7.2, 3.6, 6.1, 2.5],\n",
      "       [6.5, 3.2, 5.1, 2. ],\n",
      "       [6.4, 2.7, 5.3, 1.9],\n",
      "       [6.8, 3. , 5.5, 2.1],\n",
      "       [5.7, 2.5, 5. , 2. ],\n",
      "       [5.8, 2.8, 5.1, 2.4],\n",
      "       [6.4, 3.2, 5.3, 2.3],\n",
      "       [6.5, 3. , 5.5, 1.8],\n",
      "       [7.7, 3.8, 6.7, 2.2],\n",
      "       [7.7, 2.6, 6.9, 2.3],\n",
      "       [6. , 2.2, 5. , 1.5],\n",
      "       [6.9, 3.2, 5.7, 2.3],\n",
      "       [5.6, 2.8, 4.9, 2. ],\n",
      "       [7.7, 2.8, 6.7, 2. ],\n",
      "       [6.3, 2.7, 4.9, 1.8],\n",
      "       [6.7, 3.3, 5.7, 2.1],\n",
      "       [7.2, 3.2, 6. , 1.8],\n",
      "       [6.2, 2.8, 4.8, 1.8],\n",
      "       [6.1, 3. , 4.9, 1.8],\n",
      "       [6.4, 2.8, 5.6, 2.1],\n",
      "       [7.2, 3. , 5.8, 1.6],\n",
      "       [7.4, 2.8, 6.1, 1.9],\n",
      "       [7.9, 3.8, 6.4, 2. ],\n",
      "       [6.4, 2.8, 5.6, 2.2],\n",
      "       [6.3, 2.8, 5.1, 1.5],\n",
      "       [6.1, 2.6, 5.6, 1.4],\n",
      "       [7.7, 3. , 6.1, 2.3],\n",
      "       [6.3, 3.4, 5.6, 2.4],\n",
      "       [6.4, 3.1, 5.5, 1.8],\n",
      "       [6. , 3. , 4.8, 1.8],\n",
      "       [6.9, 3.1, 5.4, 2.1],\n",
      "       [6.7, 3.1, 5.6, 2.4],\n",
      "       [6.9, 3.1, 5.1, 2.3],\n",
      "       [5.8, 2.7, 5.1, 1.9],\n",
      "       [6.8, 3.2, 5.9, 2.3],\n",
      "       [6.7, 3.3, 5.7, 2.5],\n",
      "       [6.7, 3. , 5.2, 2.3],\n",
      "       [6.3, 2.5, 5. , 1.9],\n",
      "       [6.5, 3. , 5.2, 2. ],\n",
      "       [6.2, 3.4, 5.4, 2.3],\n",
      "       [5.9, 3. , 5.1, 1.8]]), 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), 'frame': None, 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'), 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...', 'feature_names': ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'], 'filename': '/opt/anaconda3/lib/python3.8/site-packages/sklearn/datasets/data/iris.csv'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n此处使用transform而不是fit_transform的原因：\\n如果使用fit_transform则是先对测试集的特征的数据做拟合，得到其各个特征的最大值和最小值，然后进行归一处理\\n这样就会导致得到的X_train和X_test的计算模型不同，所以不使用fit_transform，直接transform\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "# 载入Iris数据集\n",
    "iris = datasets.load_iris()\n",
    "# print(iris)\n",
    "X = iris.data # 获取iris数据集data属性的值，二维，行数等于样本数，列数等于特征数\n",
    "# print(X)\n",
    "Y = iris.target # 获取iris数据集target属性的值\n",
    "# print(Y)\n",
    "# 随机划分训练集与测试集\n",
    "X_pretrain, X_pretest, y_train, y_test = train_test_split(X, Y, test_size = 0.3)\n",
    "# 归一化\n",
    "minMax = MinMaxScaler() # 创建一个minMax对象\n",
    "X_train = minMax.fit_transform(X_pretrain)\n",
    "'''\n",
    "使用minMax对象中的fit_transform方法对训练集做归一化处理\n",
    "此处使用fit_transform是对训练集的特征的数据做拟合，得到其各个特征的最大值和最小值，然后transform进行归一化处理\n",
    "'''\n",
    "X_test = minMax.transform(X_pretest) # 使用minMax对象中的fit_transform方法对测试集做归一化处理\n",
    "'''\n",
    "此处使用transform而不是fit_transform的原因：\n",
    "如果使用fit_transform则是先对测试集的特征的数据做拟合，得到其各个特征的最大值和最小值，然后进行归一处理\n",
    "这样就会导致得到的X_train和X_test的计算模型不同，所以不使用fit_transform，直接transform\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.导入包与定义结点类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "class decisionnode: # 定义结点类\n",
    "    def __init__(self, d=None, thre=None, results=None, min_sample_number=None, lb=None, rb=None, max_label=None):\n",
    "        self.d = d # d 表示用于划分的属性索引\n",
    "        self.thre = thre # thre 表示划分所使用的阈值，每次分裂将样本集分为 2 个子集\n",
    "        self.results = results # 叶结点所代表的类别\n",
    "        self.min_sample_number = min_sample_number # 存储分支结点的最小样本量\n",
    "        self.lb = lb # 左子结点，对应属性值不大于 thre 的那些样本所在的结点\n",
    "        self.rb = rb # 右子结点，对应属性值大于 thre 的那些样本所在的结点\n",
    "        self.max_label = max_label # 记录当前结点包含的样本中样本比例最高的类别\n",
    "\n",
    "min_sample_number = 10 #设置分支结点中包含的最小样本数（即如果一个结点中包含的样本数小于该值则停止分裂）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.计算信息熵\n",
    "- 首先获取labels列表\n",
    "- 其次计算信息熵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(y):\n",
    "    # 计算信息熵，y为labels，是包含了训练集中每个样本的label值的一个列表\n",
    "    # 先获取category，即label的去重\n",
    "    if y.size > 1:  # size()用来获取元素个数。如果y中元素个数大于1则需要使用set()函数先将\n",
    "        category = list(set(y)) # 获取labels列表：y是训练集中每个样本的label值，set()将用来去重，得到去重后的标签个数，而后将其转成列表\n",
    "    else: #\n",
    "        category = [y.item()]  #\n",
    "        y = [y.item()] # 利用item()对y进行遍历，并将y中的所有元素整合为列表y\n",
    "    ent = 0\n",
    "    # 而后在\n",
    "    for label in category: # 计算信息熵\n",
    "        p = len([label_ for label_ in y if label_ == label]) / len(y) # p：样本集合中第k类样本所占的比例\n",
    "        ent += -p * math.log(p, 2) # 计算信息熵，累加\n",
    "    return ent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.计算基尼值\n",
    "- 首先获取labels列表\n",
    "- 其次计算基尼指数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gini(y):\n",
    "    # 计算基尼值，y 为 labels\n",
    "    category = list(set(y)) # 获取labels列表\n",
    "    gini = 1\n",
    "    for label in category:\n",
    "        p = len([label_ for label_ in y if label_ == label]) / len(y)\n",
    "        gini += -p * p # 基尼值：1-（两个样本为同一类的概率的累加）\n",
    "    return gini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.基于最大信息增益计算最优划分阈值\n",
    "- 利用for循环，基于信息增益，对候选划分点不断迭代，最终得到最大的信息增益和最优阈值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GainEnt_max(X, y, d):\n",
    "    # 基于最大信息增益计算最优划分阈值，X 为样本集的属性值，y 为目标值，d 是当前划分使用的属性索引\n",
    "    ent_X = entropy(y) # 信息熵\n",
    "    X_attr = X[:, d] # 获取样本集中索引d对应的属性的值\n",
    "    X_attr = list(set(X_attr)) \n",
    "    X_attr = sorted(X_attr) # d是我们使用的分类属性，是连续性数值数据，所以需要对连续型数值进行处理，这里利用sorted()函数对X_attr进行升序排列，以便后续选取中位点对连续值处理\n",
    "    Gain = 0\n",
    "    thre = 0\n",
    "    for i in range(len(X_attr) - 1):\n",
    "        thre_temp = (X_attr[i] + X_attr[i + 1]) / 2 # 候选划分点\n",
    "        y_small_index = [i_arg for i_arg in range(len(X[:, d])) if X[i_arg, d] <= thre_temp] # 获取属性值小于等于候选划分点的索引列表\n",
    "        y_big_index = [i_arg for i_arg in range(len(X[:, d])) if X[i_arg, d] > thre_temp] # 获取属性值大于候选划分点的索引列表\n",
    "        y_small = y[y_small_index] \n",
    "        y_big = y[y_big_index] \n",
    "        Gain_temp = ent_X - (len(y_small) / len(y)) * entropy(y_small) - (len(y_big) / len(y)) * entropy(y_big) # 计算信息增益\n",
    "        if Gain < Gain_temp: # 选取信息增益较大的\n",
    "            Gain = Gain_temp # 信息增益\n",
    "            thre = thre_temp # 最优阈值\n",
    "    return Gain, thre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.基于最小基尼指数计算最优划分阈值\n",
    "- 利用for循环，基于基尼指数，对候选划分点不断迭代，最终得到最小的基尼指数和最优阈值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gini_index_min(X, y, d):\n",
    "    # 基于最小基尼指数计算最优划分阈值，X 为样本集的属性值，y 为目标值，d 是当前划分使用的属性索引\n",
    "    X_attr = X[:, d] # 获取样本集中索引d对应的属性的值\n",
    "    X_attr = list(set(X_attr))\n",
    "    X_attr = sorted(X_attr) # 升序排列\n",
    "    Gini_index = 1\n",
    "    thre = 0\n",
    "    for i in range(len(X_attr) - 1):\n",
    "        thre_temp = (X_attr[i] + X_attr[i + 1]) / 2 # 候选划分点\n",
    "        y_small_index = [i_arg for i_arg in range(len(X[:, d])) if X[i_arg, d] <= thre_temp] # 获取属性值小于等于候选划分点的索引列表\n",
    "        y_big_index = [i_arg for i_arg in range(len(X[:, d])) if X[i_arg, d] > thre_temp] # 获取属性值大于候选划分点的索引列表\n",
    "        y_small = y[y_small_index]\n",
    "        y_big = y[y_big_index]\n",
    "        Gini_index_temp = (len(y_small) / len(y)) * Gini(y_small) + (len(y_big) / len(y)) * Gini(y_big) # 计算基尼指数\n",
    "        if Gini_index > Gini_index_temp: # 选取基尼指数较小的\n",
    "            Gini_index = Gini_index_temp # 基尼指数\n",
    "            thre = thre_temp # 最优阈值\n",
    "    return Gini_index, thre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.基于信息增益选择最优属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attribute_based_on_GainEnt(X, y):\n",
    "    # 基于信息增益选择最优属性，X 为样本集的属性值，y 为目标值\n",
    "    D = np.arange(len(X[0]))\n",
    "    Gain_max = 0\n",
    "    thre_ = 0\n",
    "    d_ = 0\n",
    "    for d in D:\n",
    "        Gain, thre = GainEnt_max(X, y, d) # 计算该属性的最优划分阈值\n",
    "        if Gain_max < Gain:\n",
    "            Gain_max = Gain # 选取信息增益较大的\n",
    "            thre_ = thre # 划分阈值\n",
    "            d_ = d # 属性索引\n",
    "    return Gain_max, thre_, d_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.基于基尼指数选择最优属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attribute_based_on_Giniindex(X, y):\n",
    "    # 基于基尼指数选择最优属性，X 为样本集的属性值，y 为目标值\n",
    "    D = np.arange(len(X[0]))\n",
    "    Gini_Index_Min = 1\n",
    "    thre_ = 0\n",
    "    d_ = 0\n",
    "    for d in D:\n",
    "        Gini_index, thre = Gini_index_min(X, y, d) #计算该属性的最优划分阈值\n",
    "        if Gini_Index_Min > Gini_index:\n",
    "            Gini_Index_Min = Gini_index # 选取基尼指数较小的\n",
    "            thre_ = thre # 划分阈值\n",
    "            d_ = d # 属性索引\n",
    "    return Gini_Index_Min, thre_, d_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.按照索引为 d 的属性、以 thre 为阈值将数据分为两个子集并返回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def devide_group(X, y, thre, d):\n",
    "    # 按照索引为 d 的属性、以 thre 为阈值将数据分为两个子集并返回\n",
    "    X_in_d = X[:, d]\n",
    "    x_small_index = [i_arg for i_arg in range(len(X[:, d])) if X[i_arg, d] <= thre] # 获取属性值小于等于候选划分点的索引列表\n",
    "    x_big_index = [i_arg for i_arg in range(len(X[:, d])) if X[i_arg, d] > thre] # 获取属性值大于候选划分点的索引列表\n",
    "\n",
    "    X_small = X[x_small_index] \n",
    "    y_small = y[x_small_index] \n",
    "    X_big = X[x_big_index] \n",
    "    y_big = y[x_big_index] \n",
    "    return X_small, y_small, X_big, y_big "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.计算样本集中样本占比最多的类别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxlabel(y): \n",
    "    # 计算样本集中样本占比最多的类别\n",
    "    label_ = Counter(y).most_common(1) # 利用collections.Counter中的most_common()方法得到样本中出现最多的1个类与次数\n",
    "    return label_[0][0] # 得到样本中出现次数最多的1个类别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.构建决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildtree(X, y, method='Gini'):\n",
    "    # 递归的方式构建决策树\n",
    "    if y.size > 1:\n",
    "        if method == 'Gini':\n",
    "            Gain_max, thre, d = attribute_based_on_Giniindex(X, y) # 上面定义的基于基尼指数选择最优属性的函数\n",
    "        elif method == 'GainEnt':\n",
    "            Gain_max, thre, d = attribute_based_on_GainEnt(X, y) # 上面定义的基于信息增益选择最优属性的函数\n",
    "        if len(list(y)) >= min_sample_number and ((Gain_max > 0 and method == 'GainEnt') or (Gain_max >=0 and method == 'Gini')):\n",
    "            X_small, y_small, X_big, y_big = devide_group(X, y, thre, d) # 上面定义的划分子集的函数\n",
    "            left_branch = buildtree(X_small, y_small, method=method) # 左子结点，对应属性值不大于thre的那些样本所在的结点\n",
    "            right_branch = buildtree(X_big, y_big, method=method) # 右子结点，对应属性值大于thre的那些样本所在的结点\n",
    "            max_label = maxlabel(y) # 上面定义的计算样本集中样本占比最多的类别的函数，max_label用来记录当前结点包含的样本中样本比例最高的类别\n",
    "            return decisionnode(d=d, thre=thre, min_sample_number=min_sample_number, lb=left_branch, rb=right_branch, max_label=max_label)\n",
    "        else:\n",
    "            max_label = maxlabel(y)\n",
    "            return decisionnode(results=y[0], min_sample_number=min_sample_number, max_label=max_label)\n",
    "    else:\n",
    "        max_label = maxlabel(y)\n",
    "        return decisionnode(results=y.item(), min_sample_number=min_sample_number, max_label=max_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12.利用决策树对样本进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(observation, tree): # 利用决策树对样本进行分类\n",
    "    if tree.results != None:\n",
    "        return tree.results\n",
    "    else:\n",
    "        v = observation[tree.d]\n",
    "        branch = None\n",
    "        if v > tree.thre: \n",
    "            branch = tree.rb # 对应属性值大于thre的那些样本所在的结点\n",
    "        else:\n",
    "            branch = tree.lb # 对应属性值小于等于thre的那些样本所在的结点\n",
    "        return classify(observation, branch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13.调用决策树进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CARTTree:0.96\n",
      "C3Tree:0.96\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    iris = load_iris()\n",
    "    X = iris.data # 获取iris数据集data属性的值，二维，行数等于样本数，列数等于特征数\n",
    "    y = iris.target # 获取iris数据集target属性的值\n",
    "    np.random.seed(0)\n",
    "    permutation = np.random.permutation(X.shape[0]) # 使用permutation随机打乱\n",
    "    shuffled_dataset = X[permutation, :]\n",
    "    shuffled_labels = y[permutation]\n",
    "\n",
    "    train_data = shuffled_dataset[:100, :] # 训练数据，所有列的前100行数据\n",
    "    train_label = shuffled_labels[:100] # 训练目标值，前100行\n",
    "\n",
    "    test_data = shuffled_dataset[100:150, :] # 测试数据\n",
    "    test_label = shuffled_labels[100:150] # 测试目标值\n",
    " \n",
    "    tree1 = buildtree(train_data, train_label, method='Gini') # 使用上方定义的buildtree()函数构建决策树1\n",
    "    tree2 = buildtree(train_data, train_label, method='GainEnt') # 使用上方定义的buildtree()函数构建决策树1\n",
    "\n",
    "    true_count = 0\n",
    "    for i in range(len(test_label)):\n",
    "        predict = classify(test_data[i], tree1) # 利用tree1对test_data进行分类\n",
    "        if predict == test_label[i]:\n",
    "            true_count += 1\n",
    "    acc=true_count/len(test_label) # 计算准确率\n",
    "    print(\"CARTTree:{}\".format(acc))\n",
    "    \n",
    "    true_count = 0\n",
    "    for i in range(len(test_label)):\n",
    "        predict = classify(test_data[i], tree2) # 利用tree2对test_data进行分类\n",
    "        if predict == test_label[i]:\n",
    "            true_count += 1\n",
    "    acc=true_count/len(test_label)\n",
    "    print(\"C3Tree:{}\".format(acc)) # 计算准确率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "程序运行结束后，可在屏幕上输出 CART 决策树（使用基尼指数）和 C3 决策树（使用信息增益）在测试集上的分类准确率：  \n",
    "CARTTree:0.96  \n",
    "C3Tree:0.96"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
